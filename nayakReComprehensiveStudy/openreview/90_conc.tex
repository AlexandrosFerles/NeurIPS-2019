\section{Concluding Remarks}
To summarize our work, we started with studying the effect of using latent weights on BNN. From our observations, it can be concluded that for most of the cases latent weights on BNN do not perform better than binary weights. This conclusion aligns with the authors' claim that binary weight should not be considered as an approximation of latent weights. We also present an ablation study for the choices of two hyper parameters $\tau$ and $\gamma$ for BOP optimizer. Our comprehensive experimental analysis shows the effect of each of these hyper parameters in the optimization procedure. Next, while most of the works only considered batch normalization, we introduce layer normalization as an alternative way of normalization and it shows impressive results. An ablation study on the effects of optimizer hyper parameters shows that layer normalization can provide improved results in some cases. Finally to explore the applicability of BNN in other use cases, we consider Binary AutoEncoders (BAE). The performance of BOP for BAE is then compared with BAE with Adam and AE with Adam. Though a very preliminary results with BAE are shown in our report, BOP shows better PSNR than Adam when used for training binary neural networks.
