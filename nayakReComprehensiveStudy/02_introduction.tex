\section{Introduction}
In the era of Artificial Intelligence, deep learning methods are finding themselves useful in a variety of tasks. This is not only limited to image or speech processing using supervised learning, game playing using reinforcement learning but to more complex tasks ranging from self driving car to smart personal assistants to autonomous drones helping with household work. The main challenge of using deep learning in computationally complex tasks is that it requires a big complex network which in turn requires huge memory to save its parameters and high computational power. These big networks are often trained with one or many power hungry GPUs. However, in order to enter a mass deployment of solutions, methods that can run low power devices are desirable. In order to satisfy the need of good performance of these networks but deployed with less resources in terms of memory and compute, \cite{courbariaux2015binaryconnect} proposed Binaryconnect in which the learnt parameters are limited $+1$ or $-1$. Later \cite{courbariaux2016binarized} introduced  binary neural networks (BNN) where activation also is a binarization function. The main problem of training BNN is that the gradient of binarization function is almost zero everywhere. This was mitigated by utilizing a special kind of function called straight through estimator (STE) \cite{hinton_coursera} . To train these neural networks with binary weights, a popular choice is to use Adam optimizer \cite{kingma2014adam}. During the training process, real weights are updated using one of the traditional optimizers during backpropagation and during the forward propagation, only a binarized version (sign) of these real weights (latent weights) are used for prediction and subsequent loss calculations.

% Many novel architectures are taken to improve the performance in \cite{Zhu2018,bireal_net,xnor_net,Zhuang2018} and many new optimization methods \cite{helwegen2019latent, Alizadeh2019} are explored. 

In this paper (\cite{helwegen2019latent}), the authors have developed a novel optimization method, Binary Optimizer (BOP), for training BNN. The main insight which underpins this development is the observation that latent weights are not necessary for gradient based optimization of BNNs. Instead of updating latent weights using one of the traditional optimizers, the authors proposed to use accumulated momentum to gradients in order to flip between the binary weights possible for each parameter. They empirically demonstrate the performance of BOP on CIFAR10 and Imagenet datasets.

In this study, we explore the capabilities of the proposed optimizer for training BNNs through comprehensive ablation studies.

\subsection{Outline of this study}
In order to verify the claims presented in the paper and to assess the applicability of the proposed methods to tasks other than classification, we focus on the following questions in this report.
\begin{enumerate}
    \item Does the use of latent weights instead of binary weights in BNN result in better performance?
    \item The Binary Optimizer proposed in the paper has two hyper-parameters to tune. We provide a comprehensive ablation study of the impact of these two parameters in the context of both.
    \item Most of the prior work in binary neural networks used batch normalization at the output of each layer to stabilize training. We propose layer normalization as an alternative for batch normalization and show that it can have slightly better performance for a proper choice of hyperparameters.
   
    \item Most of the previous works in binary neural networks concentrate on classification tasks, possibly due to the reduced representative power of neural networks with binarized weights. We study the applicability of binary neural networks outside to classification tasks. We present initial results on using BNNs for denosing autoencoders.
\end{enumerate}