\section{Introduction}

Knowledge distillation is a model compression technique which attempts to transfer the knowledge of large cumbersome models to smaller models. Many recent successful deep networks are extremely large and contain millions of parameters, which limits their usage to machines with more powerful hardware. For such networks to be available to a wider range of devices, model compression techniques are vital. In many cases, data availability concerning a specific task is limited, due to a variety of reasons ranging from corporate-owned datasets to the preservation of privacy of the individuals that participated in the creation of a dataset. This has in fact motivated the emerge of few/zero shot distillation approaches, where a pre-trained model can be used for distillation with little or no access to the data it was trained on. 

In this work, we reproduce the paper \textit{Zero-shot Knowledge Transfer via Adversarial Belief Matching} \cite{Micaelli2019ZeroShotKT}, where the authors present a method for distilling the knowledge of a larger pre-trained network to a smaller one, without the use of real data from the side of the student network. Our work comprises of a full re-implementation and reproduction of this method and any other methods and experiments described in this paper, including re-training the Wide Residual Network\cite{wrn} teacher networks from scratch on CIFAR10 and SVHN and reproducing the few-shot knowledge distillation via attention transfer of \cite{zagoruyko2016paying}. Additionally, we propose a modification of the main method in an attempt to yield better zero-shot knowledge transfer results. We present our results, analyze our findings, and discuss the reproducibility process of the paper with comments concerning discrepancies compared to the source code. 