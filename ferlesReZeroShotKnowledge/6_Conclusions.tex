\section{Conclusions and Future Work}
In this work, we reproduced the zero-shot knowledge transfer proposed in \cite{Micaelli2019ZeroShotKT}. Training a generator to produce images on which a student fails to match its teacher and then training the same student to mimic the decisions of its teacher on these pseudo data, ends up with similar or better performance in datasets such as SVHN and CIFAR-10. Moreover, we modified the training setting and sampled new images from the generator at each student gradient update instead of once in the beginning of the iteration. Consequently, the dataset is more diverse for the student to learn and the algorithm converges, resulting in better performance than the original method.

The initial work along with the proposed modifications leave room for further exploration and analysis. For example, the generator of a shallow network but with a more thorough designed generator, better quality adversarial features can be constructed. In-depth analysis of generated pseudo data and their diversity could also be performed, so that the resulting modified zero-shot model can provide additional insight to what effect sampling multiple batches has on the student network.

Future work can also focus on the selection of the teacher and student network architecture. Frameworks with higher representation learning capabilities compared to WRNs have emerged (a recent example would be EfficientNets\cite{effnet}) which can be alternatively used to build a better teacher network. In this direction different frameworks can be combined to match intermediate layer representation with access to the same receptive field of the original image, along with matching the distribution of the output class predictions. Another research direction, would be to further explore the usability of the fact that intermediate feature maps are also optimized through the attention transfer loss. In \cite{ltpa}, visual attention is applied to the VGG network\cite{vgg} by scaling middle and coarse layer feature maps in combination with the output feature maps to improve its performance compared to its baseline version. 
