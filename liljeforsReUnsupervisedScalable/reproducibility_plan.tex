\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% References added by Felix (not part of NeurIPS template)
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}

\title{Reproducibility plan - NeurIPS 2019 Reproducibility Challenge}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Sofia Broom√© \\
  KTH Royal Institute of Technology \\
  \texttt{sbroome@kth.se} \\
  \And
  Mohammad Moein Sorkhei \\
  KTH Royal Institute of Technology \\
  \texttt{sorkhei@kth.se} \\
  \And
  Felix Liljefors \\
  KTH Royal Institute of Technology \\
  \texttt{felixlil@kth.se} \\
}

\begin{document}

\maketitle

\section{Introduction}
We have chosen the \textbf{Replication Track} and intend to produce the following deliverables:
\begin{itemize}
    \item  A re-implementation of the encoder architecture and entire training procedure for all experiments from scratch in PyTorch, using only the research paper for instructions.
    \item Replicated results using encoder + SVM on all 128 univariate time series in the UCR repository \citep{UCRArchive2018}. This also includes three datasets that the original authors did not run on due to missing values (DodgerLoopDay, DogerLoopGame and DodgerLoopWeekend). We run on interpolated data for these three.
    \item Replicated results for the transferability experiment using an encoder trained on FordA from the UCR archive.
    \item Replicated results using encoder + SVM on all 30 multivariate time series in the UEA repository \citep{bagnall2018uea}
    \item Replicated results on the IHEPC dataset from the UCI Machine Learning Repository \citep{Dua:2019}
    \item Replicated results for the sparse labeling experiment, comparing a baseline ResNet model to the authors' method with $K=5$, for different fractions of labeled data on the TwoPatterns UCR dataset.
\end{itemize}

We have already finished nearly all implementation and run the main part of the experiments.

\bibliography{refs}

\end{document}
